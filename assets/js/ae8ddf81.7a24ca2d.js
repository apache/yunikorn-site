"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[36814],{81697:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>t,metadata:()=>i,toc:()=>l});var s=r(13274),a=r(37155);const t={id:"run_spark",title:"Run Spark Jobs",description:"How to run Spark jobs with YuniKorn",keywords:["spark"]},o=void 0,i={id:"user_guide/workloads/run_spark",title:"Run Spark Jobs",description:"How to run Spark jobs with YuniKorn",source:"@site/docs/user_guide/workloads/run_spark.md",sourceDirName:"user_guide/workloads",slug:"/user_guide/workloads/run_spark",permalink:"/docs/next/user_guide/workloads/run_spark",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{id:"run_spark",title:"Run Spark Jobs",description:"How to run Spark jobs with YuniKorn",keywords:["spark"]},sidebar:"docs",previous:{title:"Run NVIDIA GPU Jobs",permalink:"/docs/next/user_guide/workloads/run_nvidia"},next:{title:"Run Flink Jobs",permalink:"/docs/next/user_guide/workloads/run_flink"}},c={},l=[{value:"Run a Spark job with Spark Operator",id:"run-a-spark-job-with-spark-operator",level:2},{value:"Install YuniKorn",id:"install-yunikorn",level:3},{value:"Install Spark Operator",id:"install-spark-operator",level:3},{value:"Create an example application",id:"create-an-example-application",level:3},{value:"Deploy Spark job using Spark submit",id:"deploy-spark-job-using-spark-submit",level:2},{value:"Prepare the docker image for Spark",id:"prepare-the-docker-image-for-spark",level:3},{value:"Create a namespace for Spark jobs",id:"create-a-namespace-for-spark-jobs",level:3},{value:"Create service account and role binding",id:"create-service-account-and-role-binding",level:3},{value:"Submit a Spark job",id:"submit-a-spark-job",level:3},{value:"What happens behind the scenes?",id:"what-happens-behind-the-scenes",level:3},{value:"Using YuniKorn as a custom scheduler for Apache Spark on Amazon EMR on EKS",id:"using-yunikorn-as-a-custom-scheduler-for-apache-spark-on-amazon-emr-on-eks",level:2}];function p(e){const n={a:"a",admonition:"admonition",br:"br",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"run-a-spark-job-with-spark-operator",children:"Run a Spark job with Spark Operator"}),"\n",(0,s.jsxs)(n.admonition,{type:"note",children:[(0,s.jsx)(n.p,{children:"Pre-requisites:"}),(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["This tutorial assumes YuniKorn is ",(0,s.jsx)(n.a,{href:"/docs/next/",children:"installed"})," under the namespace ",(0,s.jsx)(n.code,{children:"yunikorn"})]}),"\n",(0,s.jsx)(n.li,{children:"Use spark-operator version >= 2.0 to enable support for YuniKorn gang scheduling"}),"\n"]})]}),"\n",(0,s.jsx)(n.h3,{id:"install-yunikorn",children:"Install YuniKorn"}),"\n",(0,s.jsxs)(n.p,{children:["A simple script to install YuniKorn under the namespace ",(0,s.jsx)(n.code,{children:"yunikorn"}),", refer to ",(0,s.jsx)(n.a,{href:"/docs/next/",children:"Get Started"})," for more details."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"helm repo add yunikorn https://apache.github.io/yunikorn-release\nhelm repo update\nhelm install yunikorn yunikorn/yunikorn --create-namespace --namespace yunikorn\n"})}),"\n",(0,s.jsx)(n.h3,{id:"install-spark-operator",children:"Install Spark Operator"}),"\n",(0,s.jsxs)(n.p,{children:["We should install ",(0,s.jsx)(n.code,{children:"spark-operator"})," with ",(0,s.jsx)(n.code,{children:"controller.batchScheduler.enable=true"})," and set ",(0,s.jsx)(n.code,{children:"controller.batchScheduler.default=yunikorn"})," to enable Gang Scheduling. It's optional to set the default scheduler to YuniKorn since you can specify it later on, but it's recommended to do so.",(0,s.jsx)(n.br,{}),"\n","Also, note that the total requested memory for the Spark job is the sum of memory requested for the driver and that for all executors, where each is computed as below:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Driver requested memory = ",(0,s.jsx)(n.code,{children:"spark.driver.memory"})," + ",(0,s.jsx)(n.code,{children:"spark.driver.memoryOverhead"})]}),"\n",(0,s.jsxs)(n.li,{children:["Executor requested memory = ",(0,s.jsx)(n.code,{children:"spark.executor.memory"})," + ",(0,s.jsx)(n.code,{children:"spark.executor.memoryOverhead"})," + ",(0,s.jsx)(n.code,{children:"spark.executor.pyspark.memory"})]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"helm repo add spark-operator https://kubeflow.github.io/spark-operator\nhelm repo update \nhelm install spark-operator spark-operator/spark-operator \\\n  --create-namespace \\\n  --namespace spark-operator \\\n  --set controller.batchScheduler.enable=true \\\n  --set controller.batchScheduler.default=yunikorn\n"})}),"\n",(0,s.jsx)(n.h3,{id:"create-an-example-application",children:"Create an example application"}),"\n",(0,s.jsx)(n.p,{children:"Create a Spark application to run a sample Spark Pi job."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"cat <<EOF | kubectl apply -f -\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spark-pi-yunikorn\n  namespace: default\nspec:\n  type: Scala\n  mode: cluster\n  image: spark:3.5.2\n  imagePullPolicy: IfNotPresent\n  mainClass: org.apache.spark.examples.SparkPi\n  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.2.jar\n  sparkVersion: 3.5.2\n  driver:\n    cores: 1\n    memory: 512m\n    serviceAccount: spark-operator-spark  # default service account created by spark operator\n  executor:\n    instances: 2\n    cores: 1\n    memory: 512m\n  batchScheduler: yunikorn\n  batchSchedulerOptions:\n    queue: root.default\nEOF\n"})}),"\n",(0,s.jsxs)(n.p,{children:["To view the specifics, visit ",(0,s.jsx)(n.a,{href:"https://www.kubeflow.org/docs/components/spark-operator/user-guide/yunikorn-integration/",children:"Spark operator official documentation"}),"."]}),"\n",(0,s.jsx)(n.h2,{id:"deploy-spark-job-using-spark-submit",children:"Deploy Spark job using Spark submit"}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["This document assumes you have YuniKorn and its admission-controller both installed. Please refer to\n",(0,s.jsx)(n.a,{href:"/docs/next/",children:"get started"})," to see how that is done."]})}),"\n",(0,s.jsx)(n.h3,{id:"prepare-the-docker-image-for-spark",children:"Prepare the docker image for Spark"}),"\n",(0,s.jsxs)(n.p,{children:["To run Spark on Kubernetes, you'll need the Spark docker images. You can 1) use the docker images provided by the Spark\nteam, or 2) build one from scratch.\nIf you want to build your own Spark docker image, you can find the ",(0,s.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/building-spark.html",children:"full instructions"}),"\nin the Spark documentation. Simplified steps:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Download a Spark version that has Kubernetes support, URL: ",(0,s.jsx)(n.a,{href:"https://github.com/apache/spark",children:"https://github.com/apache/spark"})]}),"\n",(0,s.jsx)(n.li,{children:"Build spark with Kubernetes support:"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"./build/mvn -Pkubernetes -DskipTests clean package\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Recommendation is to use the official images with different spark versions in the ",(0,s.jsx)(n.a,{href:"https://hub.docker.com/r/apache/spark/tags",children:"dockerhub"})]}),"\n",(0,s.jsx)(n.h3,{id:"create-a-namespace-for-spark-jobs",children:"Create a namespace for Spark jobs"}),"\n",(0,s.jsx)(n.p,{children:"Create a namespace:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-test\nEOF\n"})}),"\n",(0,s.jsx)(n.h3,{id:"create-service-account-and-role-binding",children:"Create service account and role binding"}),"\n",(0,s.jsxs)(n.p,{children:["Create service account and role bindings inside the ",(0,s.jsx)(n.code,{children:"spark-test"})," namespace:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:'cat <<EOF | kubectl apply -n spark-test -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n  namespace: spark-test\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: spark-role\n  namespace: spark-test\nrules:\n- apiGroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list", "create", "delete"]\n- apiGroups: [""]\n  resources: ["configmaps"]\n  verbs: ["get", "create", "delete"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: spark-role-binding\n  namespace: spark-test\nsubjects:\n- kind: ServiceAccount\n  name: spark\n  namespace: spark-test\nroleRef:\n  kind: Role\n  name: spark-role\n  apiGroup: rbac.authorization.k8s.io\nEOF\n'})}),"\n",(0,s.jsx)(n.admonition,{type:"note",children:(0,s.jsxs)(n.p,{children:["Do NOT use ",(0,s.jsx)(n.code,{children:"ClusterRole"})," and ",(0,s.jsx)(n.code,{children:"ClusterRoleBinding"})," to run Spark jobs in production, please configure a more fine-grained\nsecurity context for running Spark jobs. See more about how to configure proper RBAC rules ",(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/reference/access-authn-authz/rbac/",children:"here"}),"."]})}),"\n",(0,s.jsx)(n.h3,{id:"submit-a-spark-job",children:"Submit a Spark job"}),"\n",(0,s.jsx)(n.p,{children:"If this is running from local machine, you will need to start the proxy in order to talk to the api-server."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"kubectl proxy\n"})}),"\n",(0,s.jsxs)(n.p,{children:["There are official images with different spark versions in the ",(0,s.jsx)(n.a,{href:"https://hub.docker.com/r/apache/spark/tags",children:"dockerhub"}),"\nRun a simple SparkPi job, this assumes that the Spark binaries are installed locally in the ",(0,s.jsx)(n.code,{children:"/usr/local"})," directory."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-shell",metastring:"script",children:"export SPARK_HOME=/usr/local/spark/\n${SPARK_HOME}/bin/spark-submit --master k8s://http://localhost:8001 --deploy-mode cluster --name spark-pi \\\n   --master k8s://http://localhost:8001 --deploy-mode cluster --name spark-pi \\\n   --class org.apache.spark.examples.SparkPi \\\n   --conf spark.executor.instances=1 \\\n   --conf spark.kubernetes.namespace=spark-test \\\n   --conf spark.kubernetes.executor.request.cores=1 \\\n   --conf spark.kubernetes.container.image=docker.io/apache/spark:v3.3.0 \\\n   --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n   local:///opt/spark/examples/jars/spark-examples_2.12-3.3.0.jar\n"})}),"\n",(0,s.jsxs)(n.admonition,{type:"note",children:[(0,s.jsxs)(n.p,{children:["There are more options for setting the driver and executor in the ",(0,s.jsx)(n.a,{href:"https://spark.apache.org/docs/latest/running-on-kubernetes.html#configuration",children:"spark"}),".\nAssigning the applicationId and the queue path are possible."]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"--conf spark.kubernetes.executor.label.applicationId=application-spark-0001\n--conf spark.kubernetes.driver.label.applicationId=application-spark-0001\n--conf spark.kubernetes.executor.label.queue=root.default.sandbox\n--conf spark.kubernetes.driver.label.queue=root.default.sandbox\n"})})]}),"\n",(0,s.jsx)(n.p,{children:"You'll see Spark driver and executors been created on Kubernetes:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"spark-pods",src:r(69756).A+"",width:"645",height:"97"})}),"\n",(0,s.jsx)(n.p,{children:"The spark-pi result is in the driver pod."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"spark-pods",src:r(56833).A+"",width:"1081",height:"99"})}),"\n",(0,s.jsx)(n.h3,{id:"what-happens-behind-the-scenes",children:"What happens behind the scenes?"}),"\n",(0,s.jsxs)(n.p,{children:["When the Spark job is submitted to the cluster, the job is submitted to ",(0,s.jsx)(n.code,{children:"spark-test"})," namespace. The Spark driver pod will\nbe firstly created under this namespace. Since this cluster has YuniKorn admission-controller enabled, when the driver pod\nget created, the admission-controller mutates the pod's spec and injects ",(0,s.jsx)(n.code,{children:"schedulerName=yunikorn"}),", by doing this, the\ndefault K8s scheduler will skip this pod and it will be scheduled by YuniKorn instead. See how this is done by ",(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/",children:"configuring\nanother scheduler in Kubernetes"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["The default configuration has placement rule enabled, which automatically maps the ",(0,s.jsx)(n.code,{children:"spark-test"})," namespace to a YuniKorn\nqueue ",(0,s.jsx)(n.code,{children:"root.spark-test"}),". All Spark jobs submitted to this namespace will be automatically submitted to the queue first.\nTo see more about how placement rule works, please see doc ",(0,s.jsx)(n.a,{href:"/docs/next/user_guide/placement_rules",children:"placement-rules"}),". By far,\nthe namespace defines the security context of the pods, and the queue determines how the job and pods will be scheduled\nwith considering of job ordering, queue resource fairness, etc. Note, this is the simplest setup, which doesn't enforce\nthe queue capacities. The queue is considered as having unlimited capacity."]}),"\n",(0,s.jsxs)(n.p,{children:["YuniKorn reuses the Spark application ID set in label ",(0,s.jsx)(n.code,{children:"spark-app-selector"}),", and this job is submitted\nto YuniKorn and being considered as a job. The job is scheduled and running as there is sufficient resources in the cluster.\nYuniKorn allocates the driver pod to a node, binds the pod and starts all the containers. Once the driver pod gets started,\nit requests for a bunch of executor pods to run its tasks. Those pods will be created in the same namespace as well and\nscheduled by YuniKorn as well."]}),"\n",(0,s.jsx)(n.h2,{id:"using-yunikorn-as-a-custom-scheduler-for-apache-spark-on-amazon-emr-on-eks",children:"Using YuniKorn as a custom scheduler for Apache Spark on Amazon EMR on EKS"}),"\n",(0,s.jsx)(n.p,{children:"YuniKorn can be configured as a custom scheduler for Apache Spark jobs on Amazon EMR on EKS. This setup allows our\nresource management and scheduling algorithms on Kubernetes clusters."}),"\n",(0,s.jsxs)(n.p,{children:["For a detailed guide on how to set up YuniKorn with Apache Spark on Amazon EMR on EKS, please refer to the\n",(0,s.jsx)(n.a,{href:"https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/tutorial-yunikorn.html",children:"AWS EMR documentation"}),"."]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},69756:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/RunningSparkOnK8s-81f17d64fb66e633e664e62d9fc26b3e.png"},56833:(e,n,r)=>{r.d(n,{A:()=>s});const s=r.p+"assets/images/sparkResult-1100ddf2b4bcd8b3f0cce9af4e119508.png"},37155:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>i});var s=r(79474);const a={},t=s.createContext(a);function o(e){const n=s.useContext(t);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);