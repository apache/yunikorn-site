"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[8641],{58860:(e,t,n)=>{n.d(t,{xA:()=>u,yg:()=>m});var o=n(37953);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},i=Object.keys(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(o=0;o<i.length;o++)n=i[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=o.createContext({}),l=function(e){var t=o.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},u=function(e){var t=l(e.components);return o.createElement(p.Provider,{value:t},e.children)},h="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},c=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,p=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),h=l(n),c=a,m=h["".concat(p,".").concat(c)]||h[c]||d[c]||i;return n?o.createElement(m,r(r({ref:t},u),{},{components:n})):o.createElement(m,r({ref:t},u))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,r=new Array(i);r[0]=c;var s={};for(var p in t)hasOwnProperty.call(t,p)&&(s[p]=t[p]);s.originalType=e,s[h]="string"==typeof e?e:a,r[1]=s;for(var l=2;l<i;l++)r[l]=n[l];return o.createElement.apply(null,r)}return o.createElement.apply(null,n)}c.displayName="MDXCreateElement"},59957:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>l});var o=n(72994),a=(n(37953),n(58860));const i={id:"preemption",title:"Preemption"},r=void 0,s={unversionedId:"design/preemption",id:"version-1.5.0/design/preemption",title:"Preemption",description:"\x3c!--",source:"@site/versioned_docs/version-1.5.0/design/preemption.md",sourceDirName:"design",slug:"/design/preemption",permalink:"/docs/design/preemption",draft:!1,tags:[],version:"1.5.0",frontMatter:{id:"preemption",title:"Preemption"},sidebar:"docs",previous:{title:"Scheduler cache removal design",permalink:"/docs/design/cache_removal"},next:{title:"DaemonSet Scheduling using Simple Preemptor",permalink:"/docs/design/simple_preemptor"}},p={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Goals",id:"goals",level:2},{value:"Non Goals",id:"non-goals",level:2},{value:"Preemption and Priorities",id:"preemption-and-priorities",level:2},{value:"Kubernetes",id:"kubernetes",level:3},{value:"YuniKorn",id:"yunikorn",level:3},{value:"Priority limitations",id:"priority-limitations",level:3},{value:"The Laws of Preemption",id:"the-laws-of-preemption",level:2},{value:"Preemption policies are strong suggestions, not guarantees",id:"preemption-policies-are-strong-suggestions-not-guarantees",level:3},{value:"Preemption can never leave a queue lower than its guaranteed capacity",id:"preemption-can-never-leave-a-queue-lower-than-its-guaranteed-capacity",level:3},{value:"A task cannot preempt other tasks in the same application",id:"a-task-cannot-preempt-other-tasks-in-the-same-application",level:3},{value:"A task cannot trigger preemption unless its queue is under its guaranteed capacity",id:"a-task-cannot-trigger-preemption-unless-its-queue-is-under-its-guaranteed-capacity",level:3},{value:"A task cannot be preempted unless its queue is over its guaranteed capacity",id:"a-task-cannot-be-preempted-unless-its-queue-is-over-its-guaranteed-capacity",level:3},{value:"A task can only preempt a task with lower or equal priority",id:"a-task-can-only-preempt-a-task-with-lower-or-equal-priority",level:3},{value:"A task cannot preempt tasks outside its preemption fence",id:"a-task-cannot-preempt-tasks-outside-its-preemption-fence",level:3},{value:"Preemption Fence",id:"preemption-fence",level:2},{value:"Preemption Logic",id:"preemption-logic",level:2},{value:"Preemption Loop Prevention",id:"preemption-loop-prevention",level:2},{value:"Preemption Configuration",id:"preemption-configuration",level:2},{value:"PriorityClass",id:"priorityclass",level:3},{value:"Application Settings",id:"application-settings",level:3},{value:"Queue Configuration",id:"queue-configuration",level:3},{value:"Scheduler storage object changes",id:"scheduler-storage-object-changes",level:2},{value:"AllocationAsk",id:"allocationask",level:3},{value:"Allocation",id:"allocation",level:3},{value:"Queue",id:"queue",level:3}],u={toc:l},h="wrapper";function d(e){let{components:t,...i}=e;return(0,a.yg)(h,(0,o.A)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"preemption-design"},"Preemption Design"),(0,a.yg)("h2",{id:"introduction"},"Introduction"),(0,a.yg)("p",null,"This design is created to replace the preemption code that has been part of\nYuniKorn for a long time."),(0,a.yg)("p",null,"The original code was never fully tested. After the core cache removal as part\nof ",(0,a.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/YUNIKORN-317"},"YUNIKORN-317")," the\npreemption code was taken out of the scheduling path. The only changes that\nhave gone into the preemption code were compilation fixes. As part of the\npreparation for the new preemption design the old code was removed in\n",(0,a.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/YUNIKORN-1269"},"YUNIKORN-1269"),"."),(0,a.yg)("p",null,"Preemption of workloads is a feature used by almost all schedulers to help\ncritical workloads run. The criticality can be based on the fact that the\nsystem cannot work without it, i.e. DaemonSets on K8s, or based on some\nother factors like SLA or priority. Preemption for DaemonSet pods was\nimplemented as part of\n",(0,a.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/YUNIKORN-1085"},"YUNIKORN-1085"),". This\ndesign thus focuses on preemption for all other workloads."),(0,a.yg)("h2",{id:"goals"},"Goals"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Re-use existing Kubernetes objects"),(0,a.yg)("li",{parentName:"ul"},"Implement inter-queue preemption"),(0,a.yg)("li",{parentName:"ul"},"Priority knowledge inside the core scheduler to support preemption")),(0,a.yg)("h2",{id:"non-goals"},"Non Goals"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Intra-queue preemption"),(0,a.yg)("li",{parentName:"ul"},"Cross node preemption"),(0,a.yg)("li",{parentName:"ul"},"Full priority support")),(0,a.yg)("h2",{id:"preemption-and-priorities"},"Preemption and Priorities"),(0,a.yg)("h3",{id:"kubernetes"},"Kubernetes"),(0,a.yg)("p",null,"All pods for the entire cluster, across namespaces, are sorted based on\npriority for scheduling. A higher-priority will get scheduled before a\nlower-priority pod. Preemption within the Kubernetes scheduler is also\nbased solely on the priority of the pod that is being scheduled. The\nfull documentation can be found\n",(0,a.yg)("a",{parentName:"p",href:"https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/"},"here"),"."),(0,a.yg)("p",null,"The simple rule that governs both scheduling order and preemption behavior is\nthat a pod with a higher priority is more important than a pod with a lower\npriority. This crosses namespaces. There are no boundaries or limitations\nthat can be set. Pods from a namespace with a higher priority are scheduled\nearlier and could trigger the preemption of a pod from another namespace."),(0,a.yg)("p",null,"Priority of a pod is defined in the pod specification. Priority is a number,\nbut is referenced in the pod specification via a PriorityClass object. The\nobject defines a named priority and maps it to a value. In more recent\nversions (Kubernetes 1.24 and later), the PriorityClass can also contain a\npreemption policy for the scheduling phase. This preemptionPolicy only allows\nthe pod to opt-out from preempting other lower priority pods during the\nscheduling phase. It cannot, and is not designed to, alter preemption behavior\nwhile the pod is running."),(0,a.yg)("p",null,"The same pod priority is also used during the pod eviction by the kubelet on\nthe node. For example, if the node has been overloaded and runs out of memory\nor CPU, the node can evict pods. These evictions help to keep the node stable.\nPods considered for eviction are ranked on priority among some other factors."),(0,a.yg)("h3",{id:"yunikorn"},"YuniKorn"),(0,a.yg)("p",null,"The behavior in Kubernetes is all focused on the scheduling cycle. When\nlooking at preemption from the standpoint of a batch or data processing\nworkload, we need to take into account the possibility to opt-out while\nrunning. This option does not exist in the current configuration for workloads."),(0,a.yg)("p",null,"The simple approach for preemption fits in with the service type workloads\nthat Kubernetes is designed for. Services handle the scale up and or scale\ndown that could be triggered by preempting one of the pods for that service."),(0,a.yg)("p",null,"This is not the case when we look at a Spark job as an example. The driver pod\nis the manager of the other pods that belong to the same job. When the driver\ngets preempted the whole job is impacted. Preempting a driver has a follow-on\neffect that could cause multiple pods to be removed from the cluster. For a\nlarge job that could mean hundreds of pods. The impact is far larger than a\nsingle pod. Preempting a manager or originator pod should be prevented as much\nas possible."),(0,a.yg)("p",null,"The other case which we need to account for is an interactive session that\nruns on the cluster. This interactive session, like a python notebook, is a\none-off instance and restarts have a wider impact. Preempting an instance\nlike a notebook is not a desirable outcome either."),(0,a.yg)("p",null,"Building in all the smarts to know which pods to avoid is difficult, or even\nimpossible. For all these cases the option to allow a \u201cdo not preempt me\u201d flag\non the pod would be required. The design will discuss this option in the Pod\nspecification."),(0,a.yg)("h3",{id:"priority-limitations"},"Priority limitations"),(0,a.yg)("p",null,"PriorityClass objects which define the priority in a Kubernetes cluster are\ncluster-wide objects. The cluster administrator defines the objects and thus\nthe supported priorities."),(0,a.yg)("p",null,"There are no limitations on the priority that can be set on a pod. When a pod\nis submitted the only check performed is that the PriorityClass specified as\npart of the pod specification is defined. Any rogue user can create a pod with\nthe highest defined priority."),(0,a.yg)("p",null,"Limiting priorities is possible but requires an API server flag which is\ndifficult to support in public cloud environments. We cannot rely on the option\nto be available in a cluster scheduled by YuniKorn."),(0,a.yg)("h2",{id:"the-laws-of-preemption"},"The Laws of Preemption"),(0,a.yg)("p",null,'Prior knowledge of implementing preemption in the Apache YARN scheduler has\nbeen used to create this design. It forms the basis for the "laws of preemption".\nThese laws, or rules, describe overall preemption behavior.'),(0,a.yg)("p",null,"Preemption for YuniKorn is based on the hierarchical queue model and guaranteed\nresources assigned to a queue."),(0,a.yg)("p",null,"Preemption is used to get the usage of a queue to at least its guaranteed\nresource capacity. It cannot guarantee that every queue will get its\nguaranteed resource capacity. There are cases that make it impossible to get to\nthat state. Preemption therefore does not guarantee a final optimal state can\nor will be reached. The end state of the queues will normally thus be a usage\nthat is approximately the guaranteed resource capacity."),(0,a.yg)("p",null,"An example case that makes it impossible to reach the optimal state is when the\nsum of the guaranteed resources for a group of queues is larger than the current\nresources available in the cluster. It is impossible to get all queues their\nguaranteed resources. More cases like this exist in a hierarchy with maximum\nquotas set at a certain point in the hierarchy preventing the optimal state from\nbeing reached."),(0,a.yg)("p",null,"First an overview of the laws, then the explanation and rationale behind each\nof the rules:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},"Preemption policies are strong suggestions, not guarantees"),(0,a.yg)("li",{parentName:"ol"},"Preemption can never leave a queue lower than its guaranteed capacity"),(0,a.yg)("li",{parentName:"ol"},"A task cannot preempt other tasks in the same application"),(0,a.yg)("li",{parentName:"ol"},"A task cannot trigger preemption unless its queue is under its guaranteed\ncapacity"),(0,a.yg)("li",{parentName:"ol"},"A task cannot be preempted unless its queue is over its guaranteed capacity"),(0,a.yg)("li",{parentName:"ol"},"A task can only preempt a task with lower or equal priority"),(0,a.yg)("li",{parentName:"ol"},"A task cannot preempt tasks outside its preemption fence (one-way constraint)")),(0,a.yg)("p",null,"Most of the rules given around tasks and queues are there to prevent a\npreemption storm or loop. Preempting a task that could trigger a follow-up\npreemption should be avoided. The rule described should warrant against that.\nBreaking the rules could lead to unpredictable behavior either during\nscheduling or application runs."),(0,a.yg)("h3",{id:"preemption-policies-are-strong-suggestions-not-guarantees"},"Preemption policies are strong suggestions, not guarantees"),(0,a.yg)("p",null,"A pod is able to request not to be preempted. However, this is not a guarantee.\nThere could be circumstances that the scheduler might still preempt the pod.\nThis will be a last resort action if no other solution is available. This means\nthat the preemption policy is a strong suggestion, not a guarantee."),(0,a.yg)("p",null,"The example use case that could break the rule and preempt a pod that opted out\nof preemption is the DaemonSet case. The DaemonSet pod must run on the specified\nnode. If all pods that are running on that node are either other DaemonSet pods\nor pods that have opted out of preemption there is no other choice. A pod that\nopted out of preemption will be preempted."),(0,a.yg)("h3",{id:"preemption-can-never-leave-a-queue-lower-than-its-guaranteed-capacity"},"Preemption can never leave a queue lower than its guaranteed capacity"),(0,a.yg)("p",null,"Guaranteed resource capacities configured on a queue are the optimal resources\nfor the queue from a preemption point of view. Since preemption attempts to get\na queue at least its guaranteed resources, allowing preemption to reduce a queue\nbelow its guaranteed resource capacity would trigger a further round of\npreemption."),(0,a.yg)("p",null,"When a task is considered for preemption the removal of the task from the\nqueue's usage must not leave the queue below its guaranteed resource capacity.\nIf the removal of the task would drop the queue below its guarantee that task\ncannot be a candidate."),(0,a.yg)("p",null,"This could mean that a queue, even when it is above its guarantee, still does\nnot allow preempting any of the tasks in that queue."),(0,a.yg)("h3",{id:"a-task-cannot-preempt-other-tasks-in-the-same-application"},"A task cannot preempt other tasks in the same application"),(0,a.yg)("p",null,"As part of the current design we are only implementing inter-queue preemption.\nThis rule does not apply for inter-queue preemption. It is documented here to\nmake extending to intra-queue preemption possible without a rule change."),(0,a.yg)("p",null,"When a task is considered for preemption the task that triggered the preemption\ncannot be from the same application."),(0,a.yg)("h3",{id:"a-task-cannot-trigger-preemption-unless-its-queue-is-under-its-guaranteed-capacity"},"A task cannot trigger preemption unless its queue is under its guaranteed capacity"),(0,a.yg)("p",null,"Preemption attempts to get a queue at least its guaranteed resources. Allowing\npreemption for a task in a queue that is already over its guaranteed resource\ncapacity does not help reaching that goal."),(0,a.yg)("p",null,"Preemption is not the tool to get a queue to its full maximum resource\ncapacity. For that reason no tasks in a queue are allowed to trigger\npreemption in this case. Normal scheduling will handle the assignment of\nresources for queues above their guaranteed capacity."),(0,a.yg)("h3",{id:"a-task-cannot-be-preempted-unless-its-queue-is-over-its-guaranteed-capacity"},"A task cannot be preempted unless its queue is over its guaranteed capacity"),(0,a.yg)("p",null,"This is an extension to the rule that prevents a queue from going below its\nguaranteed resource capacity. If a queue is already below its guaranteed\ncapacity, no tasks running in that queue can be considered for preemption.\nThe queue can already trigger preemption of tasks from other queues. Allowing\npreemption to remove more resources from the queue would cause the queue to be\neven further from its ideal usage."),(0,a.yg)("h3",{id:"a-task-can-only-preempt-a-task-with-lower-or-equal-priority"},"A task can only preempt a task with lower or equal priority"),(0,a.yg)("p",null,"This rule is linked to the default preemption behavior of Kubernetes. There is\nhowever a slight difference when compared to the Kubernetes rule. Kubernetes\nonly allows pods to preempt other pods with a lower priority, not with equal\npriority."),(0,a.yg)("p",null,"Preemption in YuniKorn has a slightly broader application as we also use it to\nallow a queue to use its guaranteed resource capacity. The equality has been\nadded to allow pods from the same priority running in two different queues to\npreempt each other. Without that equality allowance on priorities that\nredistribution would be slow and difficult."),(0,a.yg)("h3",{id:"a-task-cannot-preempt-tasks-outside-its-preemption-fence"},"A task cannot preempt tasks outside its preemption fence"),(0,a.yg)("p",null,"This last rule is a specific multi-tenancy rule. YuniKorn allows setting up a\npreemption fence on a queue that will limit the possible tasks that can be\nconsidered for preemption. A task can only preempt another task if it runs on\na queue that is located inside the preemption fence. More on preemption\nfencing below."),(0,a.yg)("h2",{id:"preemption-fence"},"Preemption Fence"),(0,a.yg)("p",null,"In a cluster that runs workloads for multiple tenants preemption should not be\nallowed to preempt a workload from one tenant and give the resources to\nanother. A tenant in this case does not have to be a company, it could be a\ndivision or business group within the same company. Tenants map to the queue\nhierarchy. The queue hierarchy can thus cross tenant boundaries."),(0,a.yg)("p",null,"To translate this to a YuniKorn centric view: confining preemption to assess\nonly a part of the queue hierarchy should be possible. For this we want to\nintroduce a preemption fence. The preemption fence is a setting on the queue\nwhich stops preemption from looking at queues outside the fence boundary."),(0,a.yg)("p",null,"The preemption fence can be set on any queue at any point in the hierarchy.\nIt blocks traversing up the queue hierarchy when looking for a victim to\npreempt. The fence is a unidirectional fence. It prevents going out (i.e.\nhigher up the queue hierarchy), but does not prevent coming in (or down) the\nqueue hierarchy. The below diagram shows an example hierarchy with preemption\nfencing applied:"),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"preemption fencing",src:n(33490).A,width:"1579",height:"633"})),(0,a.yg)("p",null,"The example has three preemption fences defined at different levels. The\narrows describe the preemption flows."),(0,a.yg)("p",null,"First look at ",(0,a.yg)("inlineCode",{parentName:"p"},"queue A"),".\nA fence is defined on the leaf queue which means that preemption is fenced to\nthat queue. This would limit tasks to find a preemption victim within the\nqueue. This in effect turns off inter-queue preemption for the leaf\n",(0,a.yg)("inlineCode",{parentName:"p"},"queue A"),". This would be a case for intra-queue preemption only. Note that in\nthe current design intra-queue preemption is excluded, and we will not\nimplement this use case. However, we do allow the configuration to be set."),(0,a.yg)("p",null,"The second fence is set up at the ",(0,a.yg)("inlineCode",{parentName:"p"},"tenant 1")," level. This fence has an impact\non the task running in ",(0,a.yg)("inlineCode",{parentName:"p"},"queue B"),". The tasks in ",(0,a.yg)("inlineCode",{parentName:"p"},"queue B")," can find a victim\nto preempt in all queues below the ",(0,a.yg)("inlineCode",{parentName:"p"},"tenant 1")," queue (shown in the diagram with\nthe ",(0,a.yg)("em",{parentName:"p"},"yellow")," arrows)."),(0,a.yg)("p",null,"A similar setup is created for the ",(0,a.yg)("inlineCode",{parentName:"p"},"tenant 2")," queue. However, in this case\nthere can be preemption from ",(0,a.yg)("inlineCode",{parentName:"p"},"queue 1")," to ",(0,a.yg)("inlineCode",{parentName:"p"},"queue 2")," and vice-versa (shown in\nthe diagram with the ",(0,a.yg)("em",{parentName:"p"},"blue")," arrows)."),(0,a.yg)("p",null,"None of the tasks running in the queues below ",(0,a.yg)("inlineCode",{parentName:"p"},"tenant 1")," or ",(0,a.yg)("inlineCode",{parentName:"p"},"tenant 2")," can\npreempt a task from the ",(0,a.yg)("inlineCode",{parentName:"p"},"system")," queue. Tasks in the ",(0,a.yg)("inlineCode",{parentName:"p"},"system")," queue are not\nfenced. That means that a task in the ",(0,a.yg)("inlineCode",{parentName:"p"},"system")," queue can check any queue in\nthe hierarchy (shown in the diagram as the ",(0,a.yg)("em",{parentName:"p"},"red")," arrows)."),(0,a.yg)("h2",{id:"preemption-logic"},"Preemption Logic"),(0,a.yg)("p",null,"For preemption to be triggered by the default Kubernetes scheduler, the pod\nmust fit within any namespace quota and no node can be found to allocate the\npod on. Preemption tries to find a node to run the pod on and is focused on\nnodes only. All pods on a node with a lower priority are considered candidates\nfor preemption. The scheduler reduces the candidates to a victim list just\nlarge enough to fit the new pod."),(0,a.yg)("p",null,"The scheduler tracks the node undergoing preemption in the pod that triggered\nthe preemption. It does not guarantee, for a number of reasons, that the pod\nthat triggered the preemption also gets scheduled on the node the pods were\npreempted from."),(0,a.yg)("p",null,"From the YuniKorn side we already have a node focused preemption for DaemonSet\npods. A pod from a DaemonSet must run on a specific node. Preemption is\nfocused on making space on that node. YuniKorn guarantees that the pod will\nrun on that node and nothing else will be scheduled on that node until the\nDaemonSet pod is scheduled."),(0,a.yg)("p",null,"Generic preemption in YuniKorn is linked to a queue that is under its\nguaranteed quota (",(0,a.yg)("strong",{parentName:"p"},"rule 4"),"). The preemption is only triggered after a delay.\nA request must have been pending for at least the preemption delay before\npreemption will be considered for it. Instead of looking at a node the first\nstep is to find victims based on the queue that is over its guaranteed quota\n(",(0,a.yg)("strong",{parentName:"p"},"rule 5"),"). If no queues can be found within the preemption fence\n(",(0,a.yg)("strong",{parentName:"p"},"rule 7"),"), preemption immediately stops."),(0,a.yg)("p",null,"The next step is to build a list of victims. This could be from one or\nmultiple queues. Any pod with a priority higher (",(0,a.yg)("strong",{parentName:"p"},"rule 6"),") than the pod\ntriggering the preemption is discarded. The next step is to discard any pod\nwith resources larger than the over guarantee portion (",(0,a.yg)("strong",{parentName:"p"},"rule 2"),"). Proximity\nof the victim queue in the hierarchy to the originating queue that triggers\nthe preemption might be considered when we decide which victims to preempt.\nDetails will be worked out during the implementation phase."),(0,a.yg)("p",null,"Cross node preemption is not supported. Any set of victims that get preempted\nmust run on the same node. The victims are split into groups per node. A group\nof victims, combined with the allocatable space on the node, will decide if\nthe group is a viable group. Checks must be run to make sure that the pod\nwill fit the node before starting the preemption. If more than one group is\nviable a choice will need to be made. The logic for choosing a group is an\nimplementation detail."),(0,a.yg)("p",null,"After choosing a group and indirectly a node, the node is reserved for the\npod. Preemption for the victim list will be triggered and on completion the\npod will be scheduled on the node. The reservation guarantees that the pod\ntriggering the preemption will be scheduled on that node. As a side effect\nthe preempted pods that restart can also not \u201cslip back\u201d into the freed up\nspace on the node while the scheduler waits for all preempted pods to exit.\nThey could be scheduled on a different node but not stop the pod that\ntriggered the preemption from being scheduled."),(0,a.yg)("h2",{id:"preemption-loop-prevention"},"Preemption Loop Prevention"),(0,a.yg)("p",null,"The rules defined prevent looping and circular preemptions. Testing should\nbe added to cover each rule and combinations like below."),(0,a.yg)("p",null,"ReplicaSets are a good example to look at for looping and circular\npreemption. Each time a pod from a replica set is removed the ReplicaSet\ncontroller will create a new pod to make sure the set is complete. That\nauto-recreate could trigger loops if the rules are not covering all cases\ncorrectly. The case described below should be covered as part of standard\ntesting."),(0,a.yg)("p",null,"Example setup: Replica set ",(0,a.yg)("em",{parentName:"p"},"Prod Repl")," runs in queue ",(0,a.yg)("em",{parentName:"p"},"Prod"),". For testing\na replica set ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," runs in the queue ",(0,a.yg)("em",{parentName:"p"},"Test"),". Both queues belong\nto the same parent queue (they are siblings). The pods all run with the\nsame settings for priority and preemption. There is no space left on the\ncluster."),(0,a.yg)("p",null,"Example 1: ",(0,a.yg)("em",{parentName:"p"},"Prod")," is under its guaranteed quota and multiple pods of the\nreplica set are pending. All pods for the ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," set are running and\n",(0,a.yg)("em",{parentName:"p"},"Test")," is above its guaranteed resource capacity."),(0,a.yg)("p",null,"Preemption flow case 1: To make room for a ",(0,a.yg)("em",{parentName:"p"},"Prod Repl")," pod, a pod from\nthe ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," set is preempted. Both queues will end up above their\nguaranteed resource. The ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," pod is recreated. It cannot preempt\nthe ",(0,a.yg)("em",{parentName:"p"},"Prod Repl")," pod as that would leave the ",(0,a.yg)("em",{parentName:"p"},"Prod")," queue below its\nguaranteed resource capacity. The ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," pod will have to wait until\nresources become available in the cluster. The other pods for the\n",(0,a.yg)("em",{parentName:"p"},"Prod Repl")," set will also have to wait. The ",(0,a.yg)("em",{parentName:"p"},"Prod")," queue is now above its\nguaranteed resource and cannot trigger preemption anymore."),(0,a.yg)("p",null,"Preemption flow case 2: ",(0,a.yg)("em",{parentName:"p"},"Test"),", although above its guaranteed resource\ncapacity, would end up below its guarantee if a pod from ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," would be\npreempted. No preemption happens and the ",(0,a.yg)("em",{parentName:"p"},"Prod Repl")," pods will stay pending\nuntil space becomes available in the cluster."),(0,a.yg)("p",null,"Preemption flow case 3: To make room for the ",(0,a.yg)("em",{parentName:"p"},"Prod Repl")," pod a pod from the\n",(0,a.yg)("em",{parentName:"p"},"Test Repl")," set is preempted. The ",(0,a.yg)("em",{parentName:"p"},"Prod")," queue, even with the additional pod,\nis still below its guaranteed resource. The ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," pod is recreated.\n",(0,a.yg)("em",{parentName:"p"},"Prod")," is not considered as a queue with victims as it is still below its\nguaranteed resources. The ",(0,a.yg)("em",{parentName:"p"},"Test Repl")," pod will have to wait until resources\nbecome available in the cluster. The ",(0,a.yg)("em",{parentName:"p"},"Prod")," queue can trigger further\npreemptions. Preemption checks start from scratch. Depending on the state this\ncould trigger none or more preemptions from the Test queue."),(0,a.yg)("h2",{id:"preemption-configuration"},"Preemption Configuration"),(0,a.yg)("h3",{id:"priorityclass"},"PriorityClass"),(0,a.yg)("p",null,"As part of the pod specification we need to be able to provide the ability to\nopt-out of preemption. Allowing a user to specify the opt-out directly on the\npod independent of the priority causes a disconnect between Kubernetes and\nYuniKorn preemption configuration. It also removes control from the\nadministrator of the cluster to define which priorities should not be able\nto opt out."),(0,a.yg)("p",null,"One of the goals of the design is to not introduce new objects in Kubernetes.\nCombined with the above control and management of the opt-out functionality\nat a cluster level the design reuses the existing PriorityClass object.\nWe cannot and must not break existing functionality of the default Kubernetes\nscheduler when we do this. The plugin version of Kubernetes relies on the\ndefault scheduler and its behavior. Pods will use the priorityClassName just\nlike standard Kubernetes."),(0,a.yg)("p",null,"The PriorityClass object allows specifying a ",(0,a.yg)("em",{parentName:"p"},"preemptionPolicy")," with two\npossible values:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"PreemptLowerPriority"),(0,a.yg)("li",{parentName:"ul"},"Never")),(0,a.yg)("p",null,"The values are hard-coded in the core types and validations. Adding a new\nvalue is not possible without making a Kubernetes change. That means we cannot\nreuse the ",(0,a.yg)("em",{parentName:"p"},"preemptionPolicy")," field itself. The content of the field will be\npropagated to the scheduling core to allow YuniKorn to take the setting into\naccount while scheduling."),(0,a.yg)("p",null,"Just like all other objects in Kubernetes, the PriorityClass object extends\nthe base object and includes metadata. As part of the metadata we can set\nlabels and or annotations on the object. Labels are more restrictive than\nannotations. As a generic rule we prefer to use annotations over labels for\nall the extensions we are adding."),(0,a.yg)("p",null,"The proposal is to add an annotation to the PriorityClass that allows an\nopt-out of preemption. The name of the annotation would follow the same\nstructure as we use for all other annotations:\n",(0,a.yg)("inlineCode",{parentName:"p"},"yunikorn.apache.org/allow-preemption")),(0,a.yg)("p",null,"As the value of the annotation the proposal is to use a boolean value:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},'"true":\tallow the pod to be preempted,'),(0,a.yg)("li",{parentName:"ul"},'"false":\tdo not allow preemption\nIf no value is set for ',(0,a.yg)("em",{parentName:"li"},"allow-preemption")," the default value ",(0,a.yg)("em",{parentName:"li"},"true")," is used.")),(0,a.yg)("p",null,'The design comes with a caveat. The value of "',(0,a.yg)("em",{parentName:"p"},"false"),'" for the annotation\non the PriorityClass provides the possibility for a pod to request not to\nbe preempted. However, this is not a guarantee. There could be\ncircumstances that the scheduler might still preempt the pod. This will be\na last resort action if no other solution is available. This means that the\npreemption policy as defined above is a strong suggestion, not a guarantee.'),(0,a.yg)("p",null,"The example use case that could break the rule and preempt a pod with\n",(0,a.yg)("em",{parentName:"p"},"allow-preemption")," set to ",(0,a.yg)("em",{parentName:"p"},"false")," is the DaemonSet case. The DaemonSet pod\nmust run on the specified node. If all pods that are running on that node\nare either other DaemonSet pods or pods that have ",(0,a.yg)("em",{parentName:"p"},"allow-preemption")," set\nto ",(0,a.yg)("em",{parentName:"p"},"false")," there is no other choice. A pod that opted out of preemption\nwill be preempted."),(0,a.yg)("h3",{id:"application-settings"},"Application Settings"),(0,a.yg)("p",null,"Priority is part of the pod specification on Kubernetes. Every pod is\nconverted into an AllocationAsk by the k8shim. As part of the DaemonSet\npreemption that was implemented in\n",(0,a.yg)("a",{parentName:"p",href:"https://issues.apache.org/jira/browse/YUNIKORN-1085"},"YUNIKORN-1085")," the\npriority of a pod is read and set on the AllocationAsk that is created. The\npriority that is passed on currently is just a plain integer value, we do not\nneed more than that."),(0,a.yg)("p",null,"As part of preemption we need two additional values to be passed on. These two\nvalues together form the preemptionPolicy for YuniKorn. The values must be\ncommunicated as part of the AllocationAsk sent from the Kubernetes shim to the\ncore. Both values are mapped to the content of the PriorityClass described above.\nInstead of adding two separate fields, wrapping them into a new message will\nprovide more clarity."),(0,a.yg)("p",null,"The preemption policy can be processed for any AllocationAsk, even if it has\nthe ",(0,a.yg)("inlineCode",{parentName:"p"},"Originator")," flag in the message set to ",(0,a.yg)("em",{parentName:"p"},"false"),". Specifying different\npreemption behavior for different asks within an application could be important\nto allow low-cost asks to be preempted while high-cost asks opt-out."),(0,a.yg)("p",null,"The use case is again a Spark application. You want the driver pod to opt-out\nfrom preemption and, possibly, preempt other pods during scheduling. An executor\nshould do neither."),(0,a.yg)("p",null,"In protobuf message form that would look like:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre"},"message AllocationAsk {\n  ...\n  // The preemption policy for this ask\n  PreemptionPolicy preemptionPolicy = 12;\n}\n\nmessage PreemptionPolicy {\n  // Allow preemption while running\n  // Based on the YuniKorn annotation from the PriorityClass object\n  bool allowPreemption = 1;\n  // Preemption behavior while scheduling\n  // Based on the K8s preemptionPolicy from the PriorityClass object\n  string schedulerPreemption = 2;\n}\n")),(0,a.yg)("h3",{id:"queue-configuration"},"Queue Configuration"),(0,a.yg)("p",null,"Two new attributes will be added to the queue configuration object to allow\nspecifying the preemption fence type and preemption delay. Both properties,\n",(0,a.yg)("inlineCode",{parentName:"p"},"preemption.policy")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"preemption.delay"),", can be set on any queue. The\nattributes are part of the queue properties as follows:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"queues:\n  - name: fencedqueue\n    properties:\n      preemption.policy: fence\n      preemption.delay: 30s\n")),(0,a.yg)("p",null,"Properties are fully supported in child templates. Support for setting these\nproperties for dynamic queues is included as part of the existing\nfunctionality."),(0,a.yg)("p",null,"The ",(0,a.yg)("em",{parentName:"p"},"preemption.policy")," property can be set on the root queue but has no effect.\nThe root queue as the top of the hierarchy has no parent. This means preemption\ncannot traverse further up the tree. No messages will be logged if the\n",(0,a.yg)("em",{parentName:"p"},"preemption.policy")," is set on the root queue."),(0,a.yg)("p",null,"The supported values for the ",(0,a.yg)("em",{parentName:"p"},"preemption.policy")," are:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"default"),(0,a.yg)("li",{parentName:"ul"},"fence"),(0,a.yg)("li",{parentName:"ul"},"disabled")),(0,a.yg)("p",null,"If the value ",(0,a.yg)("inlineCode",{parentName:"p"},"fence")," is set on a queue, tasks running in or below the queue on\nwhich the property is set cannot preempt tasks outside the queue tree. The\nvalue ",(0,a.yg)("inlineCode",{parentName:"p"},"default")," does not limit preemption to the subtree. If no policy is set\nthe value ",(0,a.yg)("inlineCode",{parentName:"p"},"default")," is used."),(0,a.yg)("p",null,"A ",(0,a.yg)("em",{parentName:"p"},"preemption.delay")," can only be set on a leaf queue. The leaf queue contains\nthe tasks. Tasks trigger the preemption in combination with a queue being below\nits guaranteed resource capacity. Setting a preemption delay on a queue that is\nnot a leaf has no effect. No messages will be logged if a ",(0,a.yg)("em",{parentName:"p"},"preemption.delay")," is\nset on a parent queue."),(0,a.yg)("p",null,"The ",(0,a.yg)("em",{parentName:"p"},"preemption.delay")," value must be a valid Golang duration in string form. The\nvalue will be converted into a duration using ",(0,a.yg)("inlineCode",{parentName:"p"},"ParseDuration")," as defined in the\ntime package."),(0,a.yg)("p",null,"A ",(0,a.yg)("em",{parentName:"p"},"preemption.delay"),", if specified, must be larger than ",(0,a.yg)("inlineCode",{parentName:"p"},"0s"),". If no\n",(0,a.yg)("em",{parentName:"p"},"preemption.delay")," property is specified in the queue the default of ",(0,a.yg)("inlineCode",{parentName:"p"},"30s")," is\nused. If parsing of the string fails the default of ",(0,a.yg)("inlineCode",{parentName:"p"},"30s")," is used."),(0,a.yg)("h2",{id:"scheduler-storage-object-changes"},"Scheduler storage object changes"),(0,a.yg)("h3",{id:"allocationask"},"AllocationAsk"),(0,a.yg)("p",null,"In line with the changes for the communication the objects in the scheduler also\nneed to be modified to persist some of the detail communicated. For the\nAllocationAsk we need to store both new fields."),(0,a.yg)("p",null,"Proposed new fields: ",(0,a.yg)("em",{parentName:"p"},"allowPreemption")," and ",(0,a.yg)("em",{parentName:"p"},"schedulerPreemption")),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("em",{parentName:"li"},"schedulerPreemption")," is only used during the scheduling phase.  "),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("em",{parentName:"li"},"allowPreemption")," will be propagated to the resulting allocation as defined\nbelow.  ")),(0,a.yg)("h3",{id:"allocation"},"Allocation"),(0,a.yg)("p",null,"The allocation that gets created based on the AllocationAsk needs a new field.\nThis removes the need to check the underlying AllocationAsk if it allows\npreemption or not. The scheduler preemption value is not required in the\nAllocation."),(0,a.yg)("p",null,"Proposed new field: ",(0,a.yg)("em",{parentName:"p"},"allowPreemption"),"  "),(0,a.yg)("h3",{id:"queue"},"Queue"),(0,a.yg)("p",null,"The new preemption policy and delay values will need to be stored in the Queue\nobject. The configuration object does not change as we use the properties of\nthe Queue in the configuration to store them."),(0,a.yg)("p",null,"Proposed new fields: ",(0,a.yg)("em",{parentName:"p"},"preemptionPolicy")," and ",(0,a.yg)("em",{parentName:"p"},"preemptionDelay")))}d.isMDXComponent=!0},33490:(e,t,n)=>{n.d(t,{A:()=>o});const o=n.p+"assets/images/preemption_fence-ed22982d7c96f5c8c24d49c63a76aa80.png"}}]);