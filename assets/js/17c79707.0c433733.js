"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[79862],{58860:(e,r,n)=>{n.d(r,{xA:()=>u,yg:()=>m});var t=n(37953);function a(e,r,n){return r in e?Object.defineProperty(e,r,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[r]=n,e}function s(e,r){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);r&&(t=t.filter((function(r){return Object.getOwnPropertyDescriptor(e,r).enumerable}))),n.push.apply(n,t)}return n}function o(e){for(var r=1;r<arguments.length;r++){var n=null!=arguments[r]?arguments[r]:{};r%2?s(Object(n),!0).forEach((function(r){a(e,r,n[r])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):s(Object(n)).forEach((function(r){Object.defineProperty(e,r,Object.getOwnPropertyDescriptor(n,r))}))}return e}function i(e,r){if(null==e)return{};var n,t,a=function(e,r){if(null==e)return{};var n,t,a={},s=Object.keys(e);for(t=0;t<s.length;t++)n=s[t],r.indexOf(n)>=0||(a[n]=e[n]);return a}(e,r);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(t=0;t<s.length;t++)n=s[t],r.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=t.createContext({}),l=function(e){var r=t.useContext(p),n=r;return e&&(n="function"==typeof e?e(r):o(o({},r),e)),n},u=function(e){var r=l(e.components);return t.createElement(p.Provider,{value:r},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var r=e.children;return t.createElement(t.Fragment,{},r)}},h=t.forwardRef((function(e,r){var n=e.components,a=e.mdxType,s=e.originalType,p=e.parentName,u=i(e,["components","mdxType","originalType","parentName"]),c=l(n),h=a,m=c["".concat(p,".").concat(h)]||c[h]||d[h]||s;return n?t.createElement(m,o(o({ref:r},u),{},{components:n})):t.createElement(m,o({ref:r},u))}));function m(e,r){var n=arguments,a=r&&r.mdxType;if("string"==typeof e||a){var s=n.length,o=new Array(s);o[0]=h;var i={};for(var p in r)hasOwnProperty.call(r,p)&&(i[p]=r[p]);i.originalType=e,i[c]="string"==typeof e?e:a,o[1]=i;for(var l=2;l<s;l++)o[l]=n[l];return t.createElement.apply(null,o)}return t.createElement.apply(null,n)}h.displayName="MDXCreateElement"},23721:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>p,contentTitle:()=>o,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>l});var t=n(72994),a=(n(37953),n(58860));const s={id:"run_spark",title:"Run Spark Jobs",description:"How to run Spark jobs with YuniKorn",keywords:["spark"]},o=void 0,i={unversionedId:"user_guide/workloads/run_spark",id:"version-0.9.0/user_guide/workloads/run_spark",title:"Run Spark Jobs",description:"How to run Spark jobs with YuniKorn",source:"@site/versioned_docs/version-0.9.0/user_guide/workloads/run_spark.md",sourceDirName:"user_guide/workloads",slug:"/user_guide/workloads/run_spark",permalink:"/docs/0.9.0/user_guide/workloads/run_spark",draft:!1,tags:[],version:"0.9.0",frontMatter:{id:"run_spark",title:"Run Spark Jobs",description:"How to run Spark jobs with YuniKorn",keywords:["spark"]},sidebar:"version-0.9.0/docs",previous:{title:"Resource Quota Management",permalink:"/docs/0.9.0/user_guide/resource_quota_management"},next:{title:"Run Flink Jobs",permalink:"/docs/0.9.0/user_guide/workloads/run_flink"}},p={},l=[{value:"Prepare the docker image for Spark",id:"prepare-the-docker-image-for-spark",level:2},{value:"Create a namespace for Spark jobs",id:"create-a-namespace-for-spark-jobs",level:2},{value:"Submit a Spark job",id:"submit-a-spark-job",level:2},{value:"What happens behind the scenes?",id:"what-happens-behind-the-scenes",level:2}],u={toc:l},c="wrapper";function d(e){let{components:r,...s}=e;return(0,a.yg)(c,(0,t.A)({},u,s,{components:r,mdxType:"MDXLayout"}),(0,a.yg)("admonition",{type:"note"},(0,a.yg)("p",{parentName:"admonition"},"This document assumes you have YuniKorn and its admission-controller both installed. Please refer to\n",(0,a.yg)("a",{parentName:"p",href:"/docs/0.9.0/"},"get started")," to see how that is done.")),(0,a.yg)("h2",{id:"prepare-the-docker-image-for-spark"},"Prepare the docker image for Spark"),(0,a.yg)("p",null,"To run Spark on Kubernetes, you'll need the Spark docker images. You can 1) use the docker images provided by the YuniKorn\nteam, or 2) build one from scratch. If you want to build your own Spark docker image, you can"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},"Download a Spark version that has Kubernetes support, URL: ",(0,a.yg)("a",{parentName:"li",href:"https://github.com/apache/spark"},"https://github.com/apache/spark")),(0,a.yg)("li",{parentName:"ul"},"Build spark with Kubernetes support:")),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.4 -Phive -Pkubernetes -Phive-thriftserver -DskipTests package\n")),(0,a.yg)("h2",{id:"create-a-namespace-for-spark-jobs"},"Create a namespace for Spark jobs"),(0,a.yg)("p",null,"Create a namespace:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: spark-test\nEOF\n")),(0,a.yg)("p",null,"Create service account and cluster role bindings under ",(0,a.yg)("inlineCode",{parentName:"p"},"spark-test")," namespace:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},'cat <<EOF | kubectl apply -n spark-test -f -\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: spark\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: spark-cluster-role\nrules:\n- apiGroups: [""]\n  resources: ["pods"]\n  verbs: ["get", "watch", "list", "create", "delete"]\n- apiGroups: [""]\n  resources: ["configmaps"]\n  verbs: ["get", "create", "delete"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: spark-cluster-role-binding\nsubjects:\n- kind: ServiceAccount\n  name: spark\nroleRef:\n  kind: ClusterRole\n  name: spark-cluster-role\n  apiGroup: rbac.authorization.k8s.io\nEOF\n')),(0,a.yg)("admonition",{type:"note"},(0,a.yg)("p",{parentName:"admonition"},"Do NOT use ",(0,a.yg)("inlineCode",{parentName:"p"},"ClusterRole")," and ",(0,a.yg)("inlineCode",{parentName:"p"},"ClusterRoleBinding")," to run Spark jobs in production, please configure a more fine-grained\nsecurity context for running Spark jobs. See more about how to configure proper RBAC rules ",(0,a.yg)("a",{parentName:"p",href:"https://kubernetes.io/docs/reference/access-authn-authz/rbac/"},"here"),".")),(0,a.yg)("h2",{id:"submit-a-spark-job"},"Submit a Spark job"),(0,a.yg)("p",null,"If this is running from local machine, you will need to start the proxy in order to talk to the api-server."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"kubectl proxy\n")),(0,a.yg)("p",null,"Run a simple SparkPi job (this assumes that the Spark binaries are installed to ",(0,a.yg)("inlineCode",{parentName:"p"},"/usr/local")," directory)."),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-shell",metastring:"script",script:!0},"export SPARK_HOME=/usr/local/spark-2.4.4-bin-hadoop2.7/\n${SPARK_HOME}/bin/spark-submit --master k8s://http://localhost:8001 --deploy-mode cluster --name spark-pi \\\n   --master k8s://http://localhost:8001 --deploy-mode cluster --name spark-pi \\\n   --class org.apache.spark.examples.SparkPi \\\n   --conf spark.executor.instances=1 \\\n   --conf spark.kubernetes.namespace=spark-test \\\n   --conf spark.kubernetes.executor.request.cores=1 \\\n   --conf spark.kubernetes.container.image=apache/yunikorn:spark-2.4.4 \\\n   --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \\\n   local:///opt/spark/examples/jars/spark-examples_2.11-2.4.4.jar\n")),(0,a.yg)("p",null,"You'll see Spark driver and executors been created on Kubernetes:"),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"spark-pods",src:n(51852).A,width:"2724",height:"1288"})),(0,a.yg)("p",null,"You can also view the job info from YuniKorn UI. If you do not know how to access the YuniKorn UI, please read the document\n",(0,a.yg)("a",{parentName:"p",href:"/docs/0.9.0/#access-the-web-ui"},"here"),"."),(0,a.yg)("p",null,(0,a.yg)("img",{alt:"spark-jobs-on-ui",src:n(74721).A,width:"5972",height:"1732"})),(0,a.yg)("h2",{id:"what-happens-behind-the-scenes"},"What happens behind the scenes?"),(0,a.yg)("p",null,"When the Spark job is submitted to the cluster, the job is submitted to ",(0,a.yg)("inlineCode",{parentName:"p"},"spark-test")," namespace. The Spark driver pod will\nbe firstly created under this namespace. Since this cluster has YuniKorn admission-controller enabled, when the driver pod\nget created, the admission-controller mutates the pod's spec and injects ",(0,a.yg)("inlineCode",{parentName:"p"},"schedulerName=yunikorn"),", by doing this, the\ndefault K8s scheduler will skip this pod and it will be scheduled by YuniKorn instead. See how this is done by ",(0,a.yg)("a",{parentName:"p",href:"https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/"},"configuring\nanother scheduler in Kubernetes"),"."),(0,a.yg)("p",null,"The default configuration has placement rule enabled, which automatically maps the ",(0,a.yg)("inlineCode",{parentName:"p"},"spark-test")," namespace to a YuniKorn\nqueue ",(0,a.yg)("inlineCode",{parentName:"p"},"root.spark-test"),". All Spark jobs submitted to this namespace will be automatically submitted to the queue first.\nTo see more about how placement rule works, please see doc ",(0,a.yg)("a",{parentName:"p",href:"/docs/0.9.0/user_guide/placement_rules"},"placement-rules"),". By far,\nthe namespace defines the security context of the pods, and the queue determines how the job and pods will be scheduled\nwith considering of job ordering, queue resource fairness, etc. Note, this is the simplest setup, which doesn't enforce\nthe queue capacities. The queue is considered as having unlimited capacity."),(0,a.yg)("p",null,"YuniKorn reuses the Spark application ID set in label ",(0,a.yg)("inlineCode",{parentName:"p"},"spark-app-selector"),", and this job is submitted\nto YuniKorn and being considered as a job. The job is scheduled and running as there is sufficient resources in the cluster.\nYuniKorn allocates the driver pod to a node, binds the pod and starts all the containers. Once the driver pod gets started,\nit requests for a bunch of executor pods to run its tasks. Those pods will be created in the same namespace as well and\nscheduled by YuniKorn as well."))}d.isMDXComponent=!0},74721:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/spark-jobs-on-ui-21e219c2182fd987302f26a7428b9cac.png"},51852:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/spark-pods-21a4c799796031460e737650d06d3f7b.png"}}]);