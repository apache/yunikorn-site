"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[14140],{58860:(e,n,t)=>{t.d(n,{xA:()=>d,yg:()=>c});var a=t(37953);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function l(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?l(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):l(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function i(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},l=Object.keys(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(e);for(a=0;a<l.length;a++)t=l[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),u=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},d=function(e){var n=u(e.components);return a.createElement(s.Provider,{value:n},e.children)},p="mdxType",g={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,l=e.originalType,s=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),p=u(t),m=r,c=p["".concat(s,".").concat(m)]||p[m]||g[m]||l;return t?a.createElement(c,o(o({ref:n},d),{},{components:t})):a.createElement(c,o({ref:n},d))}));function c(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var l=t.length,o=new Array(l);o[0]=m;var i={};for(var s in n)hasOwnProperty.call(n,s)&&(i[s]=n[s]);i.originalType=e,i[p]="string"==typeof e?e:r,o[1]=i;for(var u=2;u<l;u++)o[u]=t[u];return a.createElement.apply(null,o)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},20207:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>g,frontMatter:()=>l,metadata:()=>i,toc:()=>u});var a=t(72994),r=(t(37953),t(58860));const l={id:"performance_tutorial",title:"Benchmarking Tutorial",keywords:["performance","tutorial"]},o=void 0,i={unversionedId:"performance/performance_tutorial",id:"version-1.1.0/performance/performance_tutorial",title:"Benchmarking Tutorial",description:"\x3c!--",source:"@site/versioned_docs/version-1.1.0/performance/performance_tutorial.md",sourceDirName:"performance",slug:"/performance/performance_tutorial",permalink:"/docs/1.1.0/performance/performance_tutorial",draft:!1,tags:[],version:"1.1.0",frontMatter:{id:"performance_tutorial",title:"Benchmarking Tutorial",keywords:["performance","tutorial"]},sidebar:"docs",previous:{title:"Evaluate YuniKorn Performance with Kubemark",permalink:"/docs/1.1.0/performance/evaluate_perf_function_with_kubemark"},next:{title:"Scheduler Metrics",permalink:"/docs/1.1.0/performance/metrics"}},s={},u=[{value:"Overview",id:"overview",level:2},{value:"Hardware",id:"hardware",level:2},{value:"1. Set /etc/sysctl.conf",id:"1-set-etcsysctlconf",level:3},{value:"2. Set /etc/security/limits.conf",id:"2-set-etcsecuritylimitsconf",level:3},{value:"Deploy workflow",id:"deploy-workflow",level:2},{value:"Setup Kubemark",id:"setup-kubemark",level:2},{value:"1. Build image",id:"1-build-image",level:3},{value:"Clone kubernetes repo, and build kubemark binary file",id:"clone-kubernetes-repo-and-build-kubemark-binary-file",level:5},{value:"Copy kubemark binary file to the image folder and build kubemark docker image",id:"copy-kubemark-binary-file-to-the-image-folder-and-build-kubemark-docker-image",level:5},{value:"2. Install Kubermark",id:"2-install-kubermark",level:3},{value:"Create kubemark namespace",id:"create-kubemark-namespace",level:5},{value:"Create configmap",id:"create-configmap",level:5},{value:"Create secret",id:"create-secret",level:5},{value:"3. Label node",id:"3-label-node",level:3},{value:"4. Deploy Kubemark",id:"4-deploy-kubemark",level:3},{value:"Deploy YuniKorn",id:"deploy-yunikorn",level:2},{value:"Install YuniKorn with helm",id:"install-yunikorn-with-helm",level:4},{value:"Configuration",id:"configuration",level:4},{value:"Install YuniKorn with local release repo",id:"install-yunikorn-with-local-release-repo",level:4},{value:"Setup Prometheus",id:"setup-prometheus",level:2},{value:"1. Download Prometheus release",id:"1-download-prometheus-release",level:3},{value:"2. Configure prometheus.yml",id:"2-configure-prometheusyml",level:3},{value:"3. Launch Prometheus",id:"3-launch-prometheus",level:3},{value:"Run tests",id:"run-tests",level:2},{value:"1. Scenarios",id:"1-scenarios",level:3},{value:"2. Build tool",id:"2-build-tool",level:3},{value:"3. Set test configuration",id:"3-set-test-configuration",level:3},{value:"Throughput case",id:"throughput-case",level:4},{value:"Node fairness case",id:"node-fairness-case",level:4},{value:"e2e perf case",id:"e2e-perf-case",level:4},{value:"4. Diagrams and logs",id:"4-diagrams-and-logs",level:3},{value:"Result diagrams and logs",id:"result-diagrams-and-logs",level:2},{value:"Collect and Observe YuniKorn metrics",id:"collect-and-observe-yunikorn-metrics",level:2},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"Kubernetes",id:"kubernetes",level:3},{value:"kubeadm",id:"kubeadm",level:4},{value:"CNI",id:"cni",level:4},{value:"Api-Server",id:"api-server",level:4},{value:"Controller-Manager",id:"controller-manager",level:4},{value:"kubelet",id:"kubelet",level:4},{value:"Summary",id:"summary",level:2}],d={toc:u},p="wrapper";function g(e){let{components:n,...l}=e;return(0,r.yg)(p,(0,a.A)({},d,l,{components:n,mdxType:"MDXLayout"}),(0,r.yg)("h2",{id:"overview"},"Overview"),(0,r.yg)("p",null,"The YuniKorn community continues to optimize the performance of the scheduler, ensuring that YuniKorn satisfies the performance requirements of large-scale batch workloads. Thus, the community has built some useful tools for performance benchmarking that can be reused across releases. This document introduces all these tools and steps to run them."),(0,r.yg)("h2",{id:"hardware"},"Hardware"),(0,r.yg)("p",null,"Be aware that performance result is highly variable depending on the underlying  hardware. All results published in the doc can only be used as references. We encourage each individual to run similar tests on their own environments in order to get a result based on your own hardware. This doc is just for demonstration purpose."),(0,r.yg)("p",null,"A list of servers being used in this test are (Huge thanks to ",(0,r.yg)("a",{parentName:"p",href:"http://www.ntcu.edu.tw/newweb/index.htm"},"National Taichung University of Education"),", ",(0,r.yg)("a",{parentName:"p",href:"http://www.ntcu.edu.tw/kclai/"},"Kuan-Chou Lai")," for providing these servers for running tests):"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Manchine Type"),(0,r.yg)("th",{parentName:"tr",align:null},"CPU"),(0,r.yg)("th",{parentName:"tr",align:null},"Memory"),(0,r.yg)("th",{parentName:"tr",align:null},"Download/upload(Mbps)"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"HP"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"36G"),(0,r.yg)("td",{parentName:"tr",align:null},"525.74/509.86")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"HP"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"30G"),(0,r.yg)("td",{parentName:"tr",align:null},"564.84/461.82")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"HP"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"30G"),(0,r.yg)("td",{parentName:"tr",align:null},"431.06/511.69")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"HP"),(0,r.yg)("td",{parentName:"tr",align:null},"24"),(0,r.yg)("td",{parentName:"tr",align:null},"32G"),(0,r.yg)("td",{parentName:"tr",align:null},"577.31/576.21")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H22"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"38G"),(0,r.yg)("td",{parentName:"tr",align:null},"432.11/4.15")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H22"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"36G"),(0,r.yg)("td",{parentName:"tr",align:null},"714.84/4.14")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H22"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"42G"),(0,r.yg)("td",{parentName:"tr",align:null},"458.38/4.13")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H22"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"42G"),(0,r.yg)("td",{parentName:"tr",align:null},"445.42/4.13")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H22"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"32G"),(0,r.yg)("td",{parentName:"tr",align:null},"400.59/4.13")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H22"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"12G"),(0,r.yg)("td",{parentName:"tr",align:null},"499.87/4.13")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"IBM blade H23"),(0,r.yg)("td",{parentName:"tr",align:null},"8"),(0,r.yg)("td",{parentName:"tr",align:null},"32G"),(0,r.yg)("td",{parentName:"tr",align:null},"468.51/4.14")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"WS660T"),(0,r.yg)("td",{parentName:"tr",align:null},"8"),(0,r.yg)("td",{parentName:"tr",align:null},"16G"),(0,r.yg)("td",{parentName:"tr",align:null},"87.73/86.30")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"ASUSPRO D640MB_M640SA"),(0,r.yg)("td",{parentName:"tr",align:null},"4"),(0,r.yg)("td",{parentName:"tr",align:null},"8G"),(0,r.yg)("td",{parentName:"tr",align:null},"92.43/93.77")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"PRO E500 G6_WS720T"),(0,r.yg)("td",{parentName:"tr",align:null},"16"),(0,r.yg)("td",{parentName:"tr",align:null},"8G"),(0,r.yg)("td",{parentName:"tr",align:null},"90/87.18")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"WS E500 G6_WS720T"),(0,r.yg)("td",{parentName:"tr",align:null},"8"),(0,r.yg)("td",{parentName:"tr",align:null},"40G"),(0,r.yg)("td",{parentName:"tr",align:null},"92.61/89.78")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"E500 G5"),(0,r.yg)("td",{parentName:"tr",align:null},"8"),(0,r.yg)("td",{parentName:"tr",align:null},"8G"),(0,r.yg)("td",{parentName:"tr",align:null},"91.34/85.84")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"WS E500 G5_WS690T"),(0,r.yg)("td",{parentName:"tr",align:null},"12"),(0,r.yg)("td",{parentName:"tr",align:null},"16G"),(0,r.yg)("td",{parentName:"tr",align:null},"92.2/93.76")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"WS E500 G5_WS690T"),(0,r.yg)("td",{parentName:"tr",align:null},"8"),(0,r.yg)("td",{parentName:"tr",align:null},"32G"),(0,r.yg)("td",{parentName:"tr",align:null},"91/89.41")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"WS E900 G4_SW980T"),(0,r.yg)("td",{parentName:"tr",align:null},"80"),(0,r.yg)("td",{parentName:"tr",align:null},"512G"),(0,r.yg)("td",{parentName:"tr",align:null},"89.24/87.97")))),(0,r.yg)("p",null,"The following steps are needed for each server, otherwise the large scale testing may fail due to the limited number of users/processes/open-files."),(0,r.yg)("h3",{id:"1-set-etcsysctlconf"},"1. Set /etc/sysctl.conf"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"kernel.pid_max=400000\nfs.inotify.max_user_instances=50000\nfs.inotify.max_user_watches=52094\n")),(0,r.yg)("h3",{id:"2-set-etcsecuritylimitsconf"},"2. Set /etc/security/limits.conf"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"* soft nproc 4000000\n* hard nproc 4000000\nroot soft nproc 4000000\nroot hard nproc 4000000\n* soft nofile 50000\n* hard nofile 50000\nroot soft nofile 50000\nroot hard nofile 50000\n")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"deploy-workflow"},"Deploy workflow"),(0,r.yg)("p",null,"Before going into the details, here are the general steps used in our tests:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#Kubernetes"},"Step 1"),": Properly configure Kubernetes API server and controller manager, then add worker nodes."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#Setup-Kubemark"},"Step 2"),": Deploy hollow pods,which will simulate worker nodes, name hollow nodes. After all hollow nodes in ready status, we need to cordon all native nodes, which are physical presence in the cluster, not the simulated nodes, to avoid we allocated test workload pod to native nodes."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#Deploy-YuniKorn"},"Step 3"),": Deploy YuniKorn using the Helm chart on the master node, and scale down the Deployment to 0 replica, and ",(0,r.yg)("a",{parentName:"li",href:"#Setup-Prometheus"},"modify the port")," in ",(0,r.yg)("inlineCode",{parentName:"li"},"prometheus.yml")," to match the port of the service."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#Run-tests"},"Step 4"),": Deploy 50k Nginx pods for testing, and the API server will create them. But since the YuniKorn scheduler Deployment has been scaled down to 0 replica, all Nginx pods will be stuck in pending."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"/docs/1.1.0/user_guide/trouble_shooting#restart-the-scheduler"},"Step 5"),": Scale up The YuniKorn Deployment back to 1 replica, and cordon the master node to avoid YuniKorn allocating Nginx pods there. In this step, YuniKorn will start collecting the metrics."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#Collect-and-Observe-YuniKorn-metrics"},"Step 6"),": Observe the metrics exposed in Prometheus UI.")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"setup-kubemark"},"Setup Kubemark"),(0,r.yg)("p",null,(0,r.yg)("a",{parentName:"p",href:"https://github.com/kubernetes/kubernetes/tree/master/test/kubemark"},"Kubemark")," is a performance testing tool which allows users to run experiments on simulated clusters. The primary use case is the scalability testing. The basic idea is to run tens or hundreds of fake kubelet nodes on one physical node in order to simulate large scale clusters. In our tests, we leverage Kubemark to simulate up to a 4K-node cluster on less than 20 physical nodes."),(0,r.yg)("h3",{id:"1-build-image"},"1. Build image"),(0,r.yg)("h5",{id:"clone-kubernetes-repo-and-build-kubemark-binary-file"},"Clone kubernetes repo, and build kubemark binary file"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"git clone https://github.com/kubernetes/kubernetes.git\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"cd kubernetes\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},'KUBE_BUILD_PLATFORMS=linux/amd64 make kubemark GOFLAGS=-v GOGCFLAGS="-N -l"\n')),(0,r.yg)("h5",{id:"copy-kubemark-binary-file-to-the-image-folder-and-build-kubemark-docker-image"},"Copy kubemark binary file to the image folder and build kubemark docker image"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"cp _output/bin/kubemark cluster/images/kubemark\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"IMAGE_TAG=v1.XX.X make build\n")),(0,r.yg)("p",null,"After this step, you can get the kubemark image which can simulate cluster node. You can upload it to Docker-Hub or just deploy it locally."),(0,r.yg)("h3",{id:"2-install-kubermark"},"2. Install Kubermark"),(0,r.yg)("h5",{id:"create-kubemark-namespace"},"Create kubemark namespace"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"kubectl create ns kubemark\n")),(0,r.yg)("h5",{id:"create-configmap"},"Create configmap"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},'kubectl create configmap node-configmap -n kubemark --from-literal=content.type="test-cluster"\n')),(0,r.yg)("h5",{id:"create-secret"},"Create secret"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"kubectl create secret generic kubeconfig --type=Opaque --namespace=kubemark --from-file=kubelet.kubeconfig={kubeconfig_file_path} --from-file=kubeproxy.kubeconfig={kubeconfig_file_path}\n")),(0,r.yg)("h3",{id:"3-label-node"},"3. Label node"),(0,r.yg)("p",null,"We need to label all native nodes, otherwise the scheduler might allocate hollow pods to other simulated hollow nodes. We can leverage Node selector in yaml to allocate hollow pods to native nodes."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"kubectl label node {node name} tag=tagName\n")),(0,r.yg)("h3",{id:"4-deploy-kubemark"},"4. Deploy Kubemark"),(0,r.yg)("p",null,"The hollow-node.yaml is down below, there are some parameters we can configure."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: hollow-node\n  namespace: kubemark\nspec:\n  replicas: 2000  # the node number you want to simulate\n  selector:\n      name: hollow-node\n  template:\n    metadata:\n      labels:\n        name: hollow-node\n    spec:\n      nodeSelector:  # leverage label to allocate to native node\n        tag: tagName  \n      initContainers:\n      - name: init-inotify-limit\n        image: docker.io/busybox:latest\n        imagePullPolicy: IfNotPresent\n        command: ['sysctl', '-w', 'fs.inotify.max_user_instances=200'] # set as same as max_user_instance in actual node \n        securityContext:\n          privileged: true\n      volumes:\n      - name: kubeconfig-volume\n        secret:\n          secretName: kubeconfig\n      - name: logs-volume\n        hostPath:\n          path: /var/log\n      containers:\n      - name: hollow-kubelet\n        image: 0yukali0/kubemark:1.20.10 # the kubemark image you build \n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 4194\n        - containerPort: 10250\n        - containerPort: 10255\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - /kubemark\n        args:\n        - --morph=kubelet\n        - --name=$(NODE_NAME)\n        - --kubeconfig=/kubeconfig/kubelet.kubeconfig\n        - --alsologtostderr\n        - --v=2\n        volumeMounts:\n        - name: kubeconfig-volume\n          mountPath: /kubeconfig\n          readOnly: true\n        - name: logs-volume\n          mountPath: /var/log\n        resources:\n          requests:    # the resource of hollow pod, can modify it.\n            cpu: 20m\n            memory: 50M\n        securityContext:\n          privileged: true\n      - name: hollow-proxy\n        image: 0yukali0/kubemark:1.20.10 # the kubemark image you build \n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        command:\n        - /kubemark\n        args:\n        - --morph=proxy\n        - --name=$(NODE_NAME)\n        - --use-real-proxier=false\n        - --kubeconfig=/kubeconfig/kubeproxy.kubeconfig\n        - --alsologtostderr\n        - --v=2\n        volumeMounts:\n        - name: kubeconfig-volume\n          mountPath: /kubeconfig\n          readOnly: true\n        - name: logs-volume\n          mountPath: /var/log\n        resources:  # the resource of hollow pod, can modify it.\n          requests:\n            cpu: 20m\n            memory: 50M\n      tolerations:\n      - effect: NoExecute\n        key: node.kubernetes.io/unreachable\n        operator: Exists\n      - effect: NoExecute\n        key: node.kubernetes.io/not-ready\n        operator: Exists\n")),(0,r.yg)("p",null,"once done editing, apply it to the cluster:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"kubectl apply -f hollow-node.yaml\n")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"deploy-yunikorn"},"Deploy YuniKorn"),(0,r.yg)("h4",{id:"install-yunikorn-with-helm"},"Install YuniKorn with helm"),(0,r.yg)("p",null,"We can install YuniKorn with Helm, please refer to this ",(0,r.yg)("a",{parentName:"p",href:"https://yunikorn.apache.org/docs/#install"},"doc"),".\nWe need to tune some parameters based on the default configuration. We recommend to clone the ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/yunikorn-release"},"release repo")," and modify the parameters in ",(0,r.yg)("inlineCode",{parentName:"p"},"value.yaml"),"."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"git clone https://github.com/apache/yunikorn-release.git\ncd helm-charts/yunikorn\n")),(0,r.yg)("h4",{id:"configuration"},"Configuration"),(0,r.yg)("p",null,"The modifications in the ",(0,r.yg)("inlineCode",{parentName:"p"},"value.yaml")," are:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"increased memory/cpu resources for the scheduler pod"),(0,r.yg)("li",{parentName:"ul"},"disabled the admission controller"),(0,r.yg)("li",{parentName:"ul"},"set the app sorting policy to FAIR")),(0,r.yg)("p",null,"please see the changes below:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"resources:\n  requests:\n    cpu: 14\n    memory: 16Gi\n  limits:\n    cpu: 14\n    memory: 16Gi\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"embedAdmissionController: false\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"configuration: |\n  partitions:\n    -\n      name: default\n      queues:\n        - name: root\n          submitacl: '*'\n          queues:\n            -\n              name: sandbox\n              properties:\n                application.sort.policy: fair\n")),(0,r.yg)("h4",{id:"install-yunikorn-with-local-release-repo"},"Install YuniKorn with local release repo"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"Helm install yunikorn . --namespace yunikorn\n")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"setup-prometheus"},"Setup Prometheus"),(0,r.yg)("p",null,"YuniKorn exposes its scheduling metrics via Prometheus. Thus, we need to set up a Prometheus server to collect these metrics."),(0,r.yg)("h3",{id:"1-download-prometheus-release"},"1. Download Prometheus release"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"wget https://github.com/prometheus/prometheus/releases/download/v2.30.3/prometheus-2.30.3.linux-amd64.tar.gz\n")),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"tar xvfz prometheus-*.tar.gz\ncd prometheus-*\n")),(0,r.yg)("h3",{id:"2-configure-prometheusyml"},"2. Configure prometheus.yml"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"global:\n  scrape_interval:     3s\n  evaluation_interval: 15s\n\nscrape_configs:\n  - job_name: 'yunikorn'\n    scrape_interval: 1s\n    metrics_path: '/ws/v1/metrics'\n    static_configs:\n    - targets: ['docker.for.mac.host.internal:9080'] \n    # 9080 is internal port, need port forward or modify 9080 to service's port\n")),(0,r.yg)("h3",{id:"3-launch-prometheus"},"3. Launch Prometheus"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"./prometheus --config.file=prometheus.yml\n")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"run-tests"},"Run tests"),(0,r.yg)("p",null,"Once the environment is setup, you are good to run workloads and collect results. YuniKorn community has some useful tools to run workloads and collect metrics, more details will be published here."),(0,r.yg)("h3",{id:"1-scenarios"},"1. Scenarios"),(0,r.yg)("p",null,"In performance tools, there are three types of tests and feedbacks."),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Test type"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"),(0,r.yg)("th",{parentName:"tr",align:null},"Diagram"),(0,r.yg)("th",{parentName:"tr",align:null},"Log"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"node fairness"),(0,r.yg)("td",{parentName:"tr",align:null},"Monitor node resource usage(allocated/capicity) with lots of pods requests"),(0,r.yg)("td",{parentName:"tr",align:null},"Exist"),(0,r.yg)("td",{parentName:"tr",align:null},"Exist")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"thourghput"),(0,r.yg)("td",{parentName:"tr",align:null},"Measure schedulers' throughput by calculating how many pods are allocated per second based on the pod start time"),(0,r.yg)("td",{parentName:"tr",align:null},"Exist"),(0,r.yg)("td",{parentName:"tr",align:null},"None")))),(0,r.yg)("h3",{id:"2-build-tool"},"2. Build tool"),(0,r.yg)("p",null,"The performance tool is available in ",(0,r.yg)("a",{parentName:"p",href:"https://github.com/apache/yunikorn-release.git"},"yunikorn release repo"),",clone the repo to your workspace. "),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"git clone https://github.com/apache/yunikorn-release.git\n")),(0,r.yg)("p",null,"Build the tool:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"cd yunikorn-release/perf-tools/\nmake build\ncd target/perf-tools-bin\n")),(0,r.yg)("p",null,"It will look like this.\n",(0,r.yg)("img",{alt:"Build-perf-tools",src:t(20638).A,width:"734",height:"236"})),(0,r.yg)("h3",{id:"3-set-test-configuration"},"3. Set test configuration"),(0,r.yg)("p",null,"Before start tests, check configuration whether meet your except.\nDefault output path is ",(0,r.yg)("inlineCode",{parentName:"p"},"/tmp"),", you can modify ",(0,r.yg)("inlineCode",{parentName:"p"},"common.outputrootpath")," to change it.\nIf you set these fields with large number to cause timeout problem, increase value in ",(0,r.yg)("inlineCode",{parentName:"p"},"common.maxwaitseconds")," to allow it."),(0,r.yg)("h4",{id:"throughput-case"},"Throughput case"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Field"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"SchedulerNames"),(0,r.yg)("td",{parentName:"tr",align:null},"List of scheduler will run the test")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"ShowNumOfLastTasks"),(0,r.yg)("td",{parentName:"tr",align:null},"Show metadata of last number of pods")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"CleanUpDelayMs"),(0,r.yg)("td",{parentName:"tr",align:null},"Controll period to refresh deployments status and print log")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"RequestConfigs"),(0,r.yg)("td",{parentName:"tr",align:null},"Set resource request and decide number of deployments and pods per deployment with ",(0,r.yg)("inlineCode",{parentName:"td"},"repeat")," and ",(0,r.yg)("inlineCode",{parentName:"td"},"numPods"))))),(0,r.yg)("p",null,"In this case,yunikorn and default scheduler will sequentially separately create ten deployments which contains fifty pods.\nIt will look like these.\n",(0,r.yg)("img",{alt:"throughputConf",src:t(84578).A,width:"277",height:"304"}),"\n",(0,r.yg)("img",{alt:"ThroughputDeployment",src:t(29275).A,width:"709",height:"210"})),(0,r.yg)("h4",{id:"node-fairness-case"},"Node fairness case"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Field"),(0,r.yg)("th",{parentName:"tr",align:null},"Description"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"SchedulerNames"),(0,r.yg)("td",{parentName:"tr",align:null},"List of schduler will run the test")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"NumPodsPerNode"),(0,r.yg)("td",{parentName:"tr",align:null},"It equals that total pods divided by nodes")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},"AllocatePercentage"),(0,r.yg)("td",{parentName:"tr",align:null},"Allow how much percentage of allocatable resource is allowed to allocate")))),(0,r.yg)("p",null,"Total number of pods will be multiplication of number of ready nodes and ",(0,r.yg)("inlineCode",{parentName:"p"},"NumPodsPerNode"),".\nIn following figure, there are thirteen ready nodes and ",(0,r.yg)("inlineCode",{parentName:"p"},"NumPodsPerNode")," is eighty.\nThere will be one thousand fourty pods created.\n",(0,r.yg)("img",{alt:"nodeFairnessConf",src:t(83512).A,width:"274",height:"144"}),"\n",(0,r.yg)("img",{alt:"nodeFairnessDeployment",src:t(50697).A,width:"696",height:"354"})),(0,r.yg)("h4",{id:"e2e-perf-case"},"e2e perf case"),(0,r.yg)("p",null,"Its field is similar to throughput one but there is only scheduler in each case.\n",(0,r.yg)("img",{alt:"scheduleTestConf",src:t(80799).A,width:"356",height:"274"}),"\n",(0,r.yg)("img",{alt:"scheduleTest",src:t(59148).A,width:"419",height:"84"})),(0,r.yg)("h3",{id:"4-diagrams-and-logs"},"4. Diagrams and logs"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"./perf-tools\n")),(0,r.yg)("p",null,"It will show result log when each case finished.\nWhen tests finished, it will look like\n",(0,r.yg)("img",{alt:"Result log",src:t(25189).A,width:"1747",height:"530"}),"\nWe can find result diagrams and logs in ",(0,r.yg)("inlineCode",{parentName:"p"},"common.outputrootpath")," which is in conf.yaml.\nRelated diagrams and logs will be like this."),(0,r.yg)("h2",{id:"result-diagrams-and-logs"},(0,r.yg)("img",{alt:"Result diagrams and logs",src:t(89629).A,width:"1613",height:"1000"})),(0,r.yg)("h2",{id:"collect-and-observe-yunikorn-metrics"},"Collect and Observe YuniKorn metrics"),(0,r.yg)("p",null,"After Prometheus is launched, YuniKorn metrics can be easily collected. Here is the ",(0,r.yg)("a",{parentName:"p",href:"/docs/1.1.0/performance/metrics"},"docs")," of YuniKorn metrics. YuniKorn tracks some key scheduling metrics which measure the latency of some critical scheduling paths. These metrics include:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"scheduling_latency_seconds:")," Latency of the main scheduling routine, in seconds."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"app_sorting_latency_seconds"),": Latency of all applications sorting, in seconds."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"node_sorting_latency_seconds"),": Latency of all nodes sorting, in seconds."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"queue_sorting_latency_seconds"),": Latency of all queues sorting, in seconds."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"container_allocation_attempt_total"),": Total number of attempts to allocate containers. State of the attempt includes ",(0,r.yg)("inlineCode",{parentName:"li"},"allocated"),", ",(0,r.yg)("inlineCode",{parentName:"li"},"rejected"),", ",(0,r.yg)("inlineCode",{parentName:"li"},"error"),", ",(0,r.yg)("inlineCode",{parentName:"li"},"released"),". Increase only.")),(0,r.yg)("p",null,"you can select and generate graph on Prometheus UI easily, such as:"),(0,r.yg)("p",null,(0,r.yg)("img",{alt:"Prometheus Metrics List",src:t(16325).A,width:"1189",height:"640"})),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"performance-tuning"},"Performance Tuning"),(0,r.yg)("h3",{id:"kubernetes"},"Kubernetes"),(0,r.yg)("p",null,"The default K8s setup has limited concurrent requests which limits the overall throughput of the cluster. In this section, we introduced a few parameters that need to be tuned up in order to increase the overall throughput of the cluster."),(0,r.yg)("h4",{id:"kubeadm"},"kubeadm"),(0,r.yg)("p",null,"Set pod-network mask"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"kubeadm init --pod-network-cidr=10.244.0.0/8\n")),(0,r.yg)("h4",{id:"cni"},"CNI"),(0,r.yg)("p",null,"Modify CNI mask and resources."),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},'  net-conf.json: |\n    {\n      "Network": "10.244.0.0/8",\n      "Backend": {\n        "Type": "vxlan"\n      }\n    }\n')),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},'  resources:\n    requests:\n      cpu: "100m"\n      memory: "200Mi"\n    limits:\n      cpu: "100m"\n      memory: "200Mi"\n')),(0,r.yg)("h4",{id:"api-server"},"Api-Server"),(0,r.yg)("p",null,"In the Kubernetes API server, we need to modify two parameters: ",(0,r.yg)("inlineCode",{parentName:"p"},"max-mutating-requests-inflight")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"max-requests-inflight"),". Those two parameters represent the API request bandwidth. Because we will generate a large amount of pod request, we need to increase those two parameters. Modify ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/kubernetes/manifest/kube-apiserver.yaml"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"--max-mutating-requests-inflight=3000\n--max-requests-inflight=3000\n")),(0,r.yg)("h4",{id:"controller-manager"},"Controller-Manager"),(0,r.yg)("p",null,"In the Kubernetes controller manager, we need to increase the value of three parameters: ",(0,r.yg)("inlineCode",{parentName:"p"},"node-cidr-mask-size"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"kube-api-burst")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"kube-api-qps"),". ",(0,r.yg)("inlineCode",{parentName:"p"},"kube-api-burst")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"kube-api-qps")," control the server side request bandwidth. ",(0,r.yg)("inlineCode",{parentName:"p"},"node-cidr-mask-size")," represents the node CIDR. it needs to be increased as well in order to scale up to thousands of nodes. "),(0,r.yg)("p",null,"Modify ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/kubernetes/manifest/kube-controller-manager.yaml"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"--node-cidr-mask-size=21 //log2(max number of pods in cluster)\n--kube-api-burst=3000\n--kube-api-qps=3000\n")),(0,r.yg)("h4",{id:"kubelet"},"kubelet"),(0,r.yg)("p",null,"In single worker node, we can run 110 pods as default. But to get higher node resource utilization, we need to add some parameters in Kubelet launch command, and restart it."),(0,r.yg)("p",null,"Modify start arg in ",(0,r.yg)("inlineCode",{parentName:"p"},"/etc/systemd/system/kubelet.service.d/10-kubeadm.conf"),", add ",(0,r.yg)("inlineCode",{parentName:"p"},"--max-Pods=300")," behind the start arg and restart"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre"},"systemctl daemon-reload\nsystemctl restart kubelet\n")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"summary"},"Summary"),(0,r.yg)("p",null,"With Kubemark and Prometheus, we can easily run benchmark testing, collect YuniKorn metrics and analyze the performance. This helps us to identify the performance bottleneck in the scheduler and further eliminate them. The YuniKorn community will continue to improve these tools in the future, and continue to gain more performance improvements."))}g.isMDXComponent=!0},83512:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/node_fairness_conf-26b753a27be63a7426b01f64ace56505.png"},20638:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/perf-tutorial-build-0ec1ae29206bea70c36aafa6b2048f81.png"},89629:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/perf-tutorial-resultDiagrams-9b91b996e4d58b60a61b8589885abe03.png"},25189:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/perf-tutorial-resultLog-3fe5dc66d81e01ef72075a5047114e7e.png"},59148:(e,n,t)=>{t.d(n,{A:()=>a});const a="data:image/png;base64,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"},80799:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/perf_e2e_test_conf-3afbe37f5dbe7062964060032337c2ea.png"},50697:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/perf_node_fairness-4c8e958d39aef3251cb538371c9486c9.png"},29275:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/perf_throughput-d33f58788b48363eeb7957d47b519e64.png"},16325:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/prometheus-3765d2ddd0d81ce31bc9c0d44046aac6.png"},84578:(e,n,t)=>{t.d(n,{A:()=>a});const a=t.p+"assets/images/throughput_conf-ead602c00872af731d09b5f44155ae7c.png"}}]);